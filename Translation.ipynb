{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Translation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pu0jfhOyXjOc"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "device_gpu = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split \n",
        "import copy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aw5RVqZQa4du",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fb3afdc-cd90-453f-ba80-958466f4b750"
      },
      "source": [
        "import nltk \n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "from nltk.tokenize import punkt\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_en=set(stopwords.words('english'))\n",
        "stop_es=set(stopwords.words('spanish'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRyTPYficl0L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "c3145553-35ed-4cc9-942d-d15b44c110fd"
      },
      "source": [
        "print(stop_es)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'habiendo', 'tendrían', 'tengan', 'fuera', 'otros', 'mío', 'tienes', 'estás', 'sentida', 'esos', 'hubiste', 'sobre', 'esto', 'contra', 'hasta', 'habrán', 'vuestra', 'cual', 'serían', 'ella', 'todo', 'nada', 'estés', 'están', 'estuviesen', 'hayáis', 'hubo', 'hubimos', 'tuvimos', 'no', 'ni', 'mis', 'su', 'estuve', 'tenga', 'ya', 'fueron', 'muy', 'suyos', 'esas', 'tenida', 'los', 'vuestro', 'sintiendo', 'otro', 'hayas', 'el', 'era', 'estará', 'a', 'habría', 'suya', 'fui', 'estado', 'sea', 'estuvo', 'tendrá', 'e', 'mucho', 'seré', 'nuestras', 'estuviese', 'serías', 'estuvisteis', 'como', 'estuvieses', 'habríamos', 'pero', 'tendrías', 'estéis', 'somos', 'tuyo', 'estuvieron', 'estamos', 'tendré', 'estemos', 'es', 'fuiste', 'habríais', 'sentido', 'lo', 'más', 'te', 'esta', 'algunos', 'eran', 'y', 'tenían', 'o', 'tendrás', 'tendrán', 'otra', 'qué', 'hubieseis', 'tuvieseis', 'vuestros', 'tuviéramos', 'estando', 'vosotras', 'tuvo', 'estas', 'estuvieran', 'eras', 'tus', 'él', 'éramos', 'por', 'nosotras', 'serás', 'tiene', 'habido', 'erais', 'hubisteis', 'sin', 'estuviera', 'fuese', 'tendremos', 'cuando', 'sentid', 'habida', 'tú', 'estabas', 'nuestra', 'tendríais', 'tuviese', 'estarían', 'he', 'porque', 'estuvieseis', 'uno', 'tened', 'tenías', 'hubieran', 'habían', 'estaríamos', 'mí', 'estaría', 'fueseis', 'tuvisteis', 'donde', 'estuvierais', 'hubieron', 'son', 'fue', 'tendría', 'habéis', 'ha', 'fueses', 'fuesen', 'ti', 'ese', 'ellas', 'una', 'tanto', 'hemos', 'hayan', 'seremos', 'tuviera', 'suyo', 'tengas', 'estuviéramos', 'estad', 'tienen', 'tenidos', 'será', 'estaremos', 'hubiera', 'teniendo', 'estuvieras', 'sentidos', 'algunas', 'hubiese', 'tuvieron', 'se', 'estados', 'tenía', 'habré', 'hubierais', 'que', 'esa', 'hay', 'tuve', 'sentidas', 'hubieras', 'desde', 'estarán', 'tuyas', 'estuvimos', 'habrá', 'al', 'sus', 'nosotros', 'habías', 'habíais', 'entre', 'habremos', 'fuéramos', 'siente', 'fueran', 'estáis', 'tuvieses', 'durante', 'han', 'tendréis', 'ante', 'estos', 'tuviésemos', 'antes', 'tengáis', 'fuerais', 'vosotros', 'ellos', 'quien', 'serán', 'estaríais', 'tengamos', 'tenido', 'quienes', 'estarás', 'haya', 'para', 'nos', 'todos', 'estoy', 'hubiésemos', 'seréis', 'fueras', 'teníamos', 'tuvierais', 'has', 'la', 'un', 'estarías', 'estaba', 'seas', 'eso', 'habíamos', 'estada', 'tuya', 'tu', 'había', 'eres', 'también', 'nuestro', 'habidas', 'seríamos', 'algo', 'mía', 'estabais', 'habrían', 'fuésemos', 'estén', 'vuestras', 'tenemos', 'tuvieras', 'tuviesen', 'estadas', 'teníais', 'fuisteis', 'poco', 'estuviste', 'tuviste', 'en', 'míos', 'estaré', 'hube', 'de', 'está', 'este', 'soy', 'me', 'estábamos', 'otras', 'yo', 'del', 'estaban', 'mías', 'tendríamos', 'tuyos', 'hubiesen', 'sois', 'unos', 'tuvieran', 'nuestros', 'estaréis', 'habréis', 'os', 'habrías', 'habrás', 'suyas', 'habidos', 'sí', 'mi', 'le', 'les', 'con', 'estar', 'las', 'esté', 'fuimos', 'tenidas', 'tenéis', 'hayamos', 'hubieses', 'estuviésemos', 'seáis', 'tengo', 'sería', 'muchos', 'seríais', 'seamos', 'sean', 'hubiéramos'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBCystYJejQA"
      },
      "source": [
        "europarl_en = open('/content/drive/My Drive/Projects/es-en/es-en/europarl-v7.es-en.en', encoding='utf-8').read().split('\\n')\n",
        "europarl_es = open('/content/drive/My Drive/Projects/es-en/es-en 2/europarl-v7.es-en.es', encoding='utf-8').read().split('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhqUz_WKeyKG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f14620e6-e627-4af3-9b40-49a147fe7eff"
      },
      "source": [
        "print(len(europarl_en))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1965735\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6o1TUt1zfNdC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "d484fd5a-503d-4d0c-bdf4-64059bcccdb8"
      },
      "source": [
        "print(europarl_es[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Reanudación del período de sesiones', 'Declaro reanudado el período de sesiones del Parlamento Europeo, interrumpido el viernes 17 de diciembre pasado, y reitero a Sus Señorías mi deseo de que hayan tenido unas buenas vacaciones.', 'Como todos han podido comprobar, el gran \"efecto del año 2000\" no se ha producido. En cambio, los ciudadanos de varios de nuestros países han sido víctimas de catástrofes naturales verdaderamente terribles.', 'Sus Señorías han solicitado un debate sobre el tema para los próximos días, en el curso de este período de sesiones.', 'A la espera de que se produzca, de acuerdo con muchos colegas que me lo han pedido, pido que hagamos un minuto de silencio en memoria de todas las víctimas de las tormentas, en los distintos países de la Unión Europea afectados.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3v95nB4-i4Mu"
      },
      "source": [
        "!python -m spacy download en\n",
        "!python -m spacy download es"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HL8zYgTTfUt0"
      },
      "source": [
        "import spacy\n",
        "import torchtext\n",
        "from torchtext.data import Field, BucketIterator, TabularDataset\n",
        "en=spacy.load('en')\n",
        "es=spacy.load('es')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPtZ2595h-_g"
      },
      "source": [
        "def tokenize_en(sentence):\n",
        "    return [tok.text for tok in en.tokenizer(sentence)]\n",
        "def tokenize_es(sentence):\n",
        "    return [tok.text for tok in es.tokenizer(sentence)]\n",
        "EN_TEXT = Field(tokenize=tokenize_en)\n",
        "ES_TEXT = Field(tokenize=tokenize_es, init_token = \"<sos>\", eos_token = \"<eos>\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1h4y8AhjusC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "2ef087f6-e249-45a6-d89f-e242260b8590"
      },
      "source": [
        "raw_data={\"English\":[line for line in europarl_en],\"Spanish\":[line for line in europarl_es]}\n",
        "df=pd.DataFrame(raw_data,columns=[\"English\",\"Spanish\"])\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English</th>\n",
              "      <th>Spanish</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Resumption of the session</td>\n",
              "      <td>Reanudación del período de sesiones</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I declare resumed the session of the European ...</td>\n",
              "      <td>Declaro reanudado el período de sesiones del P...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Although, as you will have seen, the dreaded '...</td>\n",
              "      <td>Como todos han podido comprobar, el gran \"efec...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>You have requested a debate on this subject in...</td>\n",
              "      <td>Sus Señorías han solicitado un debate sobre el...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>In the meantime, I should like to observe a mi...</td>\n",
              "      <td>A la espera de que se produzca, de acuerdo con...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             English                                            Spanish\n",
              "0                          Resumption of the session                Reanudación del período de sesiones\n",
              "1  I declare resumed the session of the European ...  Declaro reanudado el período de sesiones del P...\n",
              "2  Although, as you will have seen, the dreaded '...  Como todos han podido comprobar, el gran \"efec...\n",
              "3  You have requested a debate on this subject in...  Sus Señorías han solicitado un debate sobre el...\n",
              "4  In the meantime, I should like to observe a mi...  A la espera de que se produzca, de acuerdo con..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9N_6ZUhg0ON_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d3e16562-80c7-4d1c-a15c-1f731cada470"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1965735, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wn3C-_g0URy"
      },
      "source": [
        "df['eng_len'] = df['English'].str.count(' ')\n",
        "df['es_len'] = df['Spanish'].str.count(' ')\n",
        "df=df.query('es_len < 80 & eng_len < 80')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiYSHvvL0dMH"
      },
      "source": [
        "train,val=train_test_split(df,test_size=0.1)\n",
        "#df.to_csv('/drive/My Drive/folder_name/name_csv_file.csv')\n",
        "train.to_csv(\"/content/drive/My Drive/Projects/es-en/train.csv\",index=False)\n",
        "val.to_csv(\"/content/drive/My Drive/Projects/es-en/val.csv\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHYd-0tm1QQj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "333133bb-8ac3-4e11-f2c9-646065178560"
      },
      "source": [
        "data_fields=[('English',EN_TEXT),(\"Spanish\",ES_TEXT)]\n",
        "data_fields"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('English', <torchtext.data.field.Field at 0x7f5c237e66d8>),\n",
              " ('Spanish', <torchtext.data.field.Field at 0x7f5c237e6588>)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWkZieQ31ujl"
      },
      "source": [
        "train=TabularDataset(path='/content/drive/My Drive/Projects/es-en/train.csv',format='csv',fields=data_fields)\n",
        "val=TabularDataset(path='/content/drive/My Drive/Projects/es-en/val.csv',format='csv',fields=data_fields)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqbw6mN4ePdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "20de588c-e7fa-4396-8bd7-482679164037"
      },
      "source": [
        "type(train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torchtext.data.dataset.TabularDataset"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfSr9_jd2Np4"
      },
      "source": [
        "pre_trained_vector_type = 'glove.6B.200d'\n",
        "EN_TEXT.build_vocab(train,val,pre_trained_vector_type)\n",
        "ES_TEXT.build_vocab(train,val,pre_trained_vector_type)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04bT-jEZ7PMR"
      },
      "source": [
        "from torchtext import data\n",
        "global max_src_in_batch, max_tgt_in_batch\n",
        "def batch_size_fn(new, count, sofar):\n",
        "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
        "    global max_src_in_batch, max_tgt_in_batch\n",
        "    if count == 1:\n",
        "        max_src_in_batch = 0\n",
        "        max_tgt_in_batch = 0\n",
        "    max_src_in_batch = max(max_src_in_batch,  len(new.English))\n",
        "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.Spanish) + 2)\n",
        "    src_elements = count * max_src_in_batch\n",
        "    tgt_elements = count * max_tgt_in_batch\n",
        "    return max(src_elements, tgt_elements)\n",
        "class MyIterator(data.Iterator):\n",
        "    def create_batches(self):\n",
        "        if self.train:\n",
        "            def pool(d, random_shuffler):\n",
        "                for p in data.batch(d, self.batch_size * 100):\n",
        "                    p_batch = data.batch(\n",
        "                        sorted(p, key=self.sort_key),\n",
        "                        self.batch_size, self.batch_size_fn)\n",
        "                    for b in random_shuffler(list(p_batch)):\n",
        "                        yield b\n",
        "            self.batches = pool(self.data(), self.random_shuffler)\n",
        "            \n",
        "        else:\n",
        "            self.batches = []\n",
        "            for b in data.batch(self.data(), self.batch_size,\n",
        "                                          self.batch_size_fn):\n",
        "                self.batches.append(sorted(b, key=self.sort_key))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRguY9JV7rla"
      },
      "source": [
        "train_iter = MyIterator(train, batch_size=1300, device=device_gpu,\n",
        "                        repeat=False, sort_key= lambda x:\n",
        "                        (len(x.English), len(x.Spanish)),\n",
        "                        batch_size_fn=batch_size_fn, train=True,\n",
        "                        shuffle=True)\n",
        "batch=next(iter(train_iter))\n",
        "print(batch.English)\n",
        "type(batch.English)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlneDDlfS62a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a5cf5f43-5cfb-480a-a8d2-be6e23517939"
      },
      "source": [
        "batch.Spanish.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([28, 46])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2etaguGFdKx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "19e69f04-8d80-45de-a1a1-f9c724051187"
      },
      "source": [
        "a=[]\n",
        "b=[]\n",
        "for i in range(len(EN_TEXT.vocab)):\n",
        "    a.append(EN_TEXT.vocab.itos[i])\n",
        "for i in range(len(ES_TEXT.vocab)):\n",
        "    b.append(ES_TEXT.vocab.itos[i])\n",
        "type(a)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfOry3fDHGjC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "144342ff-0e2d-4297-a186-f89682804989"
      },
      "source": [
        "EN_TEXT.vocab.itos[3]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "','"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJc-YyFTFpz8"
      },
      "source": [
        "mix=pd.DataFrame()\n",
        "mix1=pd.DataFrame()\n",
        "mix[\"EN_TEXT\"]=a\n",
        "mix1[\"ES_TEXT\"]=b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dirhiVWOZmO9"
      },
      "source": [
        "class Embedder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "    def forward(self, x):\n",
        "        return self.embed(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eia7txkAsraB"
      },
      "source": [
        "import math\n",
        "class PositionalEncoder(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_len = 80):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        # create constant 'pe' matrix with values dependant on \n",
        "        # pos and i\n",
        "        pe = torch.zeros(max_seq_len, d_model)\n",
        "        for pos in range(max_seq_len):\n",
        "            for i in range(0, d_model, 2):\n",
        "                pe[pos, i] =math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
        "                pe[pos, i + 1] =math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "    def forward(self, x):\n",
        "        # make embeddings relatively larger\n",
        "        x = x * math.sqrt(self.d_model)\n",
        "        #add constant to embedding\n",
        "        seq_len = x.size(1)\n",
        "        x = x + Variable(self.pe[:,:seq_len], requires_grad=False).cuda()\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYAzTJD-yxG8"
      },
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, heads, d_model, dropout = 0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_k = d_model // heads\n",
        "        self.h = heads\n",
        "        self.q_linear = nn.Linear(d_model, d_model)\n",
        "        self.v_linear = nn.Linear(d_model, d_model)\n",
        "        self.k_linear = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.out = nn.Linear(d_model, d_model)\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        bs = q.size(0)\n",
        "        # perform linear operation and split into h heads\n",
        "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
        "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
        "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
        "        # transpose to get dimensions bs * h * sl * d_model\n",
        "        k = k.transpose(1,2)\n",
        "        q = q.transpose(1,2)\n",
        "        v = v.transpose(1,2)\n",
        "        # calculate attention using function we will define next\n",
        "        scores = attention(q, k, v, self.d_k, mask, self.dropout)\n",
        "        # concatenate heads and put through final linear layer\n",
        "        concat = scores.transpose(1,2).contiguous().view(bs, -1, self.d_model)\n",
        "        output = self.out(concat)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96Fb6Vk_A1-N"
      },
      "source": [
        "def attention(q, k, v, d_k, mask=None, dropout=None):\n",
        "    scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        mask = mask.unsqueeze(1)\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    scores = F.softmax(scores, dim=-1)\n",
        "    if dropout is not None:\n",
        "        scores = dropout(scores)\n",
        "    output = torch.matmul(scores, v)\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JMcqmW5EAJ7"
      },
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff=2048, dropout = 0.1):\n",
        "        super().__init__() \n",
        "        # We set d_ff as a default to 2048\n",
        "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(F.relu(self.linear_1(x)))\n",
        "        x = self.linear_2(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJoSq1O3FBfs"
      },
      "source": [
        "class Norm(nn.Module):\n",
        "    def __init__(self, d_model, eps = 1e-6):\n",
        "        super().__init__()\n",
        "        self.size = d_model\n",
        "        # create two learnable parameters to calibrate normalisation\n",
        "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
        "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
        "        self.eps = eps\n",
        "    def forward(self, x):\n",
        "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True))/(x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
        "        return norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yDU1M5EGx71"
      },
      "source": [
        "# build an encoder layer with one multi-head attention layer and one # feed-forward layer\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, heads, dropout = 0.1):\n",
        "        super().__init__()\n",
        "        self.norm_1 = Norm(d_model)\n",
        "        self.norm_2 = Norm(d_model)\n",
        "        self.attn = MultiHeadAttention(heads, d_model)\n",
        "        self.ff = FeedForward(d_model)\n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "    def forward(self, x, mask):\n",
        "        x2 = self.norm_1(x)\n",
        "        x = x + self.dropout_1(self.attn(x2,x2,x2,mask))\n",
        "        x2 = self.norm_2(x)\n",
        "        x = x + self.dropout_2(self.ff(x2))\n",
        "        return x\n",
        "# build a decoder layer with two multi-head attention layers and\n",
        "# one feed-forward layer\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm_1 = Norm(d_model)\n",
        "        self.norm_2 = Norm(d_model)\n",
        "        self.norm_3 = Norm(d_model)\n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "        self.dropout_3 = nn.Dropout(dropout)\n",
        "        self.attn_1 = MultiHeadAttention(heads, d_model)\n",
        "        self.attn_2 = MultiHeadAttention(heads, d_model)\n",
        "        self.ff = FeedForward(d_model).to(device_gpu)\n",
        "    def forward(self, x, e_outputs, src_mask, trg_mask):\n",
        "        x2 = self.norm_1(x)\n",
        "        x = x + self.dropout_1(self.attn_1(x2, x2, x2, trg_mask))\n",
        "        x2 = self.norm_2(x)\n",
        "        x = x + self.dropout_2(self.attn_2(x2, e_outputs, e_outputs,src_mask))\n",
        "        x2 = self.norm_3(x)\n",
        "        x = x + self.dropout_3(self.ff(x2))\n",
        "        return x\n",
        "# We can then build a convenient cloning function that can generate multiple layers:\n",
        "def get_clones(module, N):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-YeArf4pdAs"
      },
      "source": [
        "class Embedder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "    def forward(self, x):\n",
        "        return self.embed(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cawz52-mpd57",
        "outputId": "fc4c25fa-3118-407c-90a8-8066614f3261"
      },
      "source": [
        "net=Embedder(100,512)\n",
        "a=torch.Tensor(1,100)\n",
        "net(a.type(torch.LongTensor))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 1.4537, -1.4817, -0.7653,  ...,  0.6204,  1.4153, -0.2327],\n",
              "         [ 1.4537, -1.4817, -0.7653,  ...,  0.6204,  1.4153, -0.2327],\n",
              "         [ 1.4537, -1.4817, -0.7653,  ...,  0.6204,  1.4153, -0.2327],\n",
              "         ...,\n",
              "         [ 1.4537, -1.4817, -0.7653,  ...,  0.6204,  1.4153, -0.2327],\n",
              "         [ 1.4537, -1.4817, -0.7653,  ...,  0.6204,  1.4153, -0.2327],\n",
              "         [ 1.4537, -1.4817, -0.7653,  ...,  0.6204,  1.4153, -0.2327]]],\n",
              "       grad_fn=<EmbeddingBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAnFS7GiG9_A"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, N, heads):\n",
        "        super().__init__()\n",
        "        self.N = N\n",
        "        self.embed = Embedder(vocab_size, d_model)\n",
        "        self.pe = PositionalEncoder(d_model)\n",
        "        self.layers = get_clones(EncoderLayer(d_model, heads), N)\n",
        "        self.norm = Norm(d_model)\n",
        "    def forward(self, src, mask):\n",
        "        x = self.embed(src)\n",
        "        x = self.pe(x)\n",
        "        for i in range(N):\n",
        "            x = self.layers[i](x, mask)\n",
        "            \n",
        "        return self.norm(x)\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, N, heads):\n",
        "        super().__init__()\n",
        "        self.N = N\n",
        "        self.embed = Embedder(vocab_size, d_model)\n",
        "        self.pe = PositionalEncoder(d_model)\n",
        "        self.layers = get_clones(DecoderLayer(d_model, heads), N)\n",
        "        self.norm = Norm(d_model)\n",
        "    def forward(self, trg, e_outputs, src_mask, trg_mask):\n",
        "        x = self.embed(trg)\n",
        "        x = self.pe(x)\n",
        "        for i in range(self.N):\n",
        "          \n",
        "          #print(e_outputs)\n",
        "          x = self.layers[i](x, e_outputs, src_mask, trg_mask)\n",
        "          #print(x,e_outputs, src_mask, trg_mask)\n",
        "        return self.norm(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJef1G_bH7ho"
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab, trg_vocab, d_model, N, heads):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(src_vocab, d_model, N, heads)\n",
        "        self.decoder = Decoder(trg_vocab, d_model, N, heads)\n",
        "        self.out = nn.Linear(d_model, trg_vocab)\n",
        "    def forward(self, src, trg, src_mask, trg_mask):\n",
        "        e_outputs = self.encoder(src, src_mask)\n",
        "        #print(e_outputs)\n",
        "        d_output = self.decoder(trg, e_outputs, src_mask, trg_mask)\n",
        "        output = self.out(d_output)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meFruoe4IJiX"
      },
      "source": [
        "d_model = 512\n",
        "heads = 8\n",
        "N = 6\n",
        "src_vocab = len(EN_TEXT.vocab)\n",
        "trg_vocab = len(ES_TEXT.vocab)\n",
        "model = Transformer(src_vocab, trg_vocab, d_model, N, heads).to(device_gpu)\n",
        "for p in model.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "optim = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08_ClS41haGY"
      },
      "source": [
        "def nopeak_mask(size):\n",
        "    np_mask = np.triu(np.ones((1, size, size),dtype='uint8'),k=1)\n",
        "    np_mask =  Variable(torch.from_numpy(np_mask) == 0)\n",
        "    return np_mask\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsS7UwB6lGdL"
      },
      "source": [
        "def create_masks(src, trg):\n",
        "    src_pad = EN_TEXT.vocab.stoi['<pad>']\n",
        "    src_mask = (src != src_pad).unsqueeze(-2)\n",
        "    src_mask=src_mask.to(device_gpu)\n",
        "\n",
        "    if trg is not None:\n",
        "        trg_pad=ES_TEXT.vocab.stoi['<pad>']\n",
        "        trg_mask = (trg != trg_pad).unsqueeze(-2)\n",
        "        trg_mask=trg_mask.to(device_gpu)\n",
        "        size = trg.size(1) # get seq_len for matrix\n",
        "        \n",
        "        np_mask = nopeak_mask(size)\n",
        "        np_mask=np_mask.to(device_gpu)\n",
        "        trg_mask = trg_mask & np_mask\n",
        "        \n",
        "    else:\n",
        "        trg_mask = None\n",
        "    return src_mask, trg_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dfs-D_jIN9D"
      },
      "source": [
        "def train_model(epochs, print_every=100):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    start = time.time()\n",
        "    temp = start\n",
        "    \n",
        "    total_loss = 0\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "       \n",
        "        for i, batch in enumerate(train_iter):\n",
        "            src = batch.English.transpose(0,1)\n",
        "            trg = batch.Spanish.transpose(0,1)\n",
        "            \n",
        "            # the French sentence we input has all words except\n",
        "            # the last, as it is using each word to predict the next\n",
        "            \n",
        "            trg_input = trg[:, :-1]\n",
        "            \n",
        "            # the words we are trying to predict\n",
        "            \n",
        "            targets = trg[:, 1:].contiguous().view(-1)\n",
        "            \n",
        "            # create function to make masks using mask code above\n",
        "            \n",
        "            src_mask, trg_mask = create_masks(src, trg_input)\n",
        "            #print(trg,trg_mask)\n",
        "            preds = model(src, trg_input, src_mask, trg_mask)\n",
        "            \n",
        "            optim.zero_grad()\n",
        "            \n",
        "            loss = F.cross_entropy(preds.view(-1, preds.size(-1)),targets)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            \n",
        "            total_loss += loss.data[0]\n",
        "            if (i + 1) % print_every == 0:\n",
        "                loss_avg = total_loss / print_every\n",
        "                print(\"time = %dm, epoch %d, iter = %d, loss = %.3f%ds per %d iters\" % ((time.time() - start) // 60,\n",
        "                epoch + 1, i + 1, loss_avg, time.time() - temp,\n",
        "                print_every))\n",
        "                total_loss = 0\n",
        "                temp = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMxCKKDRfaof",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "outputId": "65102d33-4174-45a8-887e-0238578ca168"
      },
      "source": [
        "import time\n",
        "from torch.autograd import Variable\n",
        "t=train_model(epochs=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-370b4ab9f2f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-40-06370c208591>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(epochs, print_every)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2316\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2317\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1533\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'log_softmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1535\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1536\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1537\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 926.00 MiB (GPU 0; 11.17 GiB total capacity; 10.33 GiB already allocated; 70.81 MiB free; 10.79 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6_puAt2oUhz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}